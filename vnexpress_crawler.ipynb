{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9544e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫Øt ƒë·∫ßu crawl 5 b√†i b√°o ch√≠nh tr·ªã t·ª´ VnExpress...\n",
      "T√¨m th·∫•y 5 b√†i b√°o. B·∫Øt ƒë·∫ßu crawl...\n",
      "ƒêang crawl b√†i 1/5: https://vnexpress.net/ong-nguyen-van-tho-lam-pho-chu-tich-thuong-truc-ubnd-tp-hcm-4910346.html\n",
      "‚úì Crawl th√†nh c√¥ng: √îng Nguy·ªÖn VƒÉn Th·ªç l√†m Ph√≥ ch·ªß t·ªãch th∆∞·ªùng tr·ª±c UB...\n",
      "ƒêang crawl b√†i 2/5: https://vnexpress.net/viet-nam-thai-lan-thuc-day-hop-tac-quoc-phong-thuc-chat-di-vao-chieu-sau-4910310.html\n",
      "‚úì Crawl th√†nh c√¥ng: Vi·ªát Nam - Th√°i Lan th√∫c ƒë·∫©y h·ª£p t√°c qu·ªëc ph√≤ng th...\n",
      "ƒêang crawl b√†i 3/5: https://vnexpress.net/tong-bi-thu-yeu-cau-sua-chinh-sach-tien-luong-can-bo-cong-chuc-phu-hop-voi-mo-hinh-moi-4910138.html\n",
      "‚úì Crawl th√†nh c√¥ng: T·ªïng B√≠ th∆∞ y√™u c·∫ßu s·ª≠a ch√≠nh s√°ch ti·ªÅn l∆∞∆°ng c√°n ...\n",
      "ƒêang crawl b√†i 4/5: https://vnexpress.net/cong-bo-6-bai-toan-lon-ve-khoa-hoc-cong-nghe-trong-linh-vuc-quoc-phong-4909909.html\n",
      "‚úì Crawl th√†nh c√¥ng: C√¥ng b·ªë 6 b√†i to√°n l·ªõn v·ªÅ khoa h·ªçc c√¥ng ngh·ªá trong...\n",
      "ƒêang crawl b√†i 5/5: https://vnexpress.net/noi-bien-voi-rung-quang-ngai-hoi-tu-nhieu-loi-the-4909395.html\n",
      "‚úì Crawl th√†nh c√¥ng: 'N·ªëi bi·ªÉn v·ªõi r·ª´ng, Qu·∫£ng Ng√£i h·ªôi t·ª• nhi·ªÅu l·ª£i th...\n",
      "\n",
      "================================================================================\n",
      "T√ìM T·∫ÆT K·∫æT QU·∫¢ CRAWL TIN T·ª®C CH√çNH TR·ªä VNEXPRESS\n",
      "================================================================================\n",
      "\n",
      "üî∏ B√ÄI 1:\n",
      "Ti√™u ƒë·ªÅ: √îng Nguy·ªÖn VƒÉn Th·ªç l√†m Ph√≥ ch·ªß t·ªãch th∆∞·ªùng tr·ª±c UBND TP HCM\n",
      "URL: https://vnexpress.net/ong-nguyen-van-tho-lam-pho-chu-tich-thuong-truc-ubnd-tp-hcm-4910346.html\n",
      "Th·ªùi gian: Th·ª© s√°u, 4/7/2025, 20:51 (GMT+7)\n",
      "T√≥m t·∫Øt: √îng Nguy·ªÖn VƒÉn Th·ªç, 57 tu·ªïi, ƒë∆∞·ª£c ph√¢n c√¥ng l√†m Ph√≥ ch·ªß t·ªãch th∆∞·ªùng tr·ª±c UBND TP HCM, nhi·ªám k·ª≥ 2021-...\n",
      "T·ªï ch·ª©c ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: Ch√≠nh ph·ªß, Qu·ªëc h·ªôi\n",
      "ƒê·ªãa danh ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: TP HCM, th√†nh ph·ªë Nguy·ªÖn \n",
      "------------------------------------------------------------\n",
      "\n",
      "üî∏ B√ÄI 2:\n",
      "Ti√™u ƒë·ªÅ: Vi·ªát Nam - Th√°i Lan th√∫c ƒë·∫©y h·ª£p t√°c qu·ªëc ph√≤ng th·ª±c ch·∫•t, ƒëi v√†o chi·ªÅu s√¢u\n",
      "URL: https://vnexpress.net/viet-nam-thai-lan-thuc-day-hop-tac-quoc-phong-thuc-chat-di-vao-chieu-sau-4910310.html\n",
      "Th·ªùi gian: Th·ª© s√°u, 4/7/2025, 19:01 (GMT+7)\n",
      "T√≥m t·∫Øt: ƒê·∫°i t∆∞·ªõng Nguy·ªÖn T√¢n C∆∞∆°ng kh·∫≥ng ƒë·ªãnh Vi·ªát Nam coi tr·ªçng quan h·ªá h·ª£p t√°c v·ªõi c√°c n∆∞·ªõc ASEAN, trong ƒë...\n",
      "T·ªï ch·ª©c ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: ASEAN, Qu·ªëc ph√≤ng\n",
      "ƒê·ªãa danh ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: H√† N·ªôi, Vi·ªát Nam\n",
      "------------------------------------------------------------\n",
      "\n",
      "üî∏ B√ÄI 3:\n",
      "Ti√™u ƒë·ªÅ: T·ªïng B√≠ th∆∞ y√™u c·∫ßu s·ª≠a ch√≠nh s√°ch ti·ªÅn l∆∞∆°ng c√°n b·ªô, c√¥ng ch·ª©c ph√π h·ª£p v·ªõi m√¥ h√¨nh m·ªõi\n",
      "URL: https://vnexpress.net/tong-bi-thu-yeu-cau-sua-chinh-sach-tien-luong-can-bo-cong-chuc-phu-hop-voi-mo-hinh-moi-4910138.html\n",
      "Th·ªùi gian: Th·ª© s√°u, 4/7/2025, 14:58 (GMT+7)\n",
      "T√≥m t·∫Øt: T·ªïng B√≠ th∆∞ T√¥ L√¢m y√™u c·∫ßu ti·∫øp t·ª•c r√† so√°t, s·ª≠a ƒë·ªïi, b·ªï sung quy ƒë·ªãnh v·ªÅ ch√≠nh s√°ch ti·ªÅn l∆∞∆°ng, ph·ª•...\n",
      "L√£nh ƒë·∫°o ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: T√¥ L√¢m\n",
      "T·ªï ch·ª©c ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: B·ªô Ch√≠nh tr·ªã, Ch√≠nh tr·ªã\n",
      "------------------------------------------------------------\n",
      "\n",
      "üî∏ B√ÄI 4:\n",
      "Ti√™u ƒë·ªÅ: C√¥ng b·ªë 6 b√†i to√°n l·ªõn v·ªÅ khoa h·ªçc c√¥ng ngh·ªá trong lƒ©nh v·ª±c Qu·ªëc ph√≤ng\n",
      "URL: https://vnexpress.net/cong-bo-6-bai-toan-lon-ve-khoa-hoc-cong-nghe-trong-linh-vuc-quoc-phong-4909909.html\n",
      "Th·ªùi gian: Th·ª© s√°u, 4/7/2025, 09:32 (GMT+7)\n",
      "T√≥m t·∫Øt: B·ªô Qu·ªëc ph√≤ng v·ª´a c√¥ng b·ªë Danh m·ª•c b√†i to√°n l·ªõn v·ªÅ khoa h·ªçc, c√¥ng ngh·ªá, ƒë·ªïi m·ªõi s√°ng t·∫°o v√† chuy·ªÉn ƒë...\n",
      "T·ªï ch·ª©c ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: Qu·ªëc ph√≤ng v·ª´a c√¥ng b·ªë , Qu·ªëc ph√≤ng x√°c ƒë·ªãnh l√† tr·ªçng y·∫øu\n",
      "ƒê·ªãa danh ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: Vi·ªát Nam\n",
      "------------------------------------------------------------\n",
      "\n",
      "üî∏ B√ÄI 5:\n",
      "Ti√™u ƒë·ªÅ: 'N·ªëi bi·ªÉn v·ªõi r·ª´ng, Qu·∫£ng Ng√£i h·ªôi t·ª• nhi·ªÅu l·ª£i th·∫ø'\n",
      "URL: https://vnexpress.net/noi-bien-voi-rung-quang-ngai-hoi-tu-nhieu-loi-the-4909395.html\n",
      "Th·ªùi gian: Th·ª© s√°u, 4/7/2025, 06:00 (GMT+7)\n",
      "T√≥m t·∫Øt: Qu·∫£ng Ng√£i m·ªõi c√≥ ƒë·ªãa th·∫ø \"t·ª±a s∆°n, h∆∞·ªõng th·ªßy\", ƒëa d·∫°ng t·ª´ c∆° c·∫•u kinh t·∫ø ƒë·∫øn b·∫£n s·∫Øc vƒÉn h√≥a, theo...\n",
      "ƒê·ªãa danh ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: t·ªânh Nguy·ªÖn , t·ªânh Qu·∫£ng , Qu·∫£ng Ng√£i\n",
      "------------------------------------------------------------\n",
      "ƒê√£ l∆∞u d·ªØ li·ªáu v√†o file vnexpress_political_news.csv\n",
      "\n",
      "‚úÖ Ho√†n th√†nh crawl 5 b√†i b√°o!\n",
      "üìÑ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o file 'vnexpress_political_news.json' v√† 'vnexpress_political_news.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "\n",
    "class VnExpressCrawler:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.base_url = \"https://vnexpress.net\"\n",
    "    \n",
    "    def get_political_news_links(self, limit=5):\n",
    "        \"\"\"L·∫•y danh s√°ch link b√†i b√°o ch√≠nh tr·ªã m·ªõi nh·∫•t\"\"\"\n",
    "        try:\n",
    "            url = \"https://vnexpress.net/thoi-su/chinh-tri\"\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # T√¨m c√°c link b√†i b√°o (c·∫≠p nh·∫≠t selector ph√π h·ª£p v·ªõi c·∫•u tr√∫c m·ªõi)\n",
    "            article_links = []\n",
    "            # Th·ª≠ t√¨m c√°c th·∫ª article c√≥ class 'item-news'\n",
    "            articles = soup.find_all('article', class_='item-news')\n",
    "            for article in articles:\n",
    "                link_tag = article.find('a', href=True)\n",
    "                if link_tag:\n",
    "                    full_url = link_tag['href']\n",
    "                    if not full_url.startswith('http'):\n",
    "                        full_url = self.base_url + full_url\n",
    "                    article_links.append(full_url)\n",
    "                if len(article_links) >= limit:\n",
    "                    break\n",
    "            # N·∫øu kh√¥ng t√¨m th·∫•y, fallback sang selector c≈©\n",
    "            if not article_links:\n",
    "                articles = soup.find_all('h3', class_='title-news')\n",
    "                for article in articles:\n",
    "                    link_tag = article.find('a')\n",
    "                    if link_tag and link_tag.get('href'):\n",
    "                        full_url = link_tag['href']\n",
    "                        if not full_url.startswith('http'):\n",
    "                            full_url = self.base_url + full_url\n",
    "                        article_links.append(full_url)\n",
    "                    if len(article_links) >= limit:\n",
    "                        break\n",
    "            return article_links\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"L·ªói khi l·∫•y danh s√°ch b√†i b√°o: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_article_info(self, url):\n",
    "        \"\"\"Crawl th√¥ng tin chi ti·∫øt t·ª´ m·ªôt b√†i b√°o\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # L·∫•y ti√™u ƒë·ªÅ\n",
    "            title = \"\"\n",
    "            title_tag = soup.find('h1', class_='title-detail')\n",
    "            if title_tag:\n",
    "                title = title_tag.get_text(strip=True)\n",
    "            \n",
    "            # L·∫•y t√≥m t·∫Øt\n",
    "            summary = \"\"\n",
    "            summary_tag = soup.find('p', class_='description')\n",
    "            if summary_tag:\n",
    "                summary = summary_tag.get_text(strip=True)\n",
    "            \n",
    "            # L·∫•y th·ªùi gian ƒëƒÉng\n",
    "            publish_time = \"\"\n",
    "            time_tag = soup.find('span', class_='date')\n",
    "            if time_tag:\n",
    "                publish_time = time_tag.get_text(strip=True)\n",
    "            \n",
    "            # L·∫•y n·ªôi dung ch√≠nh\n",
    "            content = \"\"\n",
    "            content_div = soup.find('article', class_='fck_detail')\n",
    "            if content_div:\n",
    "                paragraphs = content_div.find_all('p', class_='Normal')\n",
    "                content = ' '.join([p.get_text(strip=True) for p in paragraphs[:3]])  # L·∫•y 3 ƒëo·∫°n ƒë·∫ßu\n",
    "            \n",
    "            # Tr√≠ch xu·∫•t c√°c ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p (t√™n ng∆∞·ªùi, ch·ª©c v·ª•)\n",
    "            mentioned_entities = self.extract_entities(title + \" \" + summary + \" \" + content)\n",
    "            # Lo·∫°i b·ªè tr∆∞·ªùng positions kh·ªèi mentioned_entities\n",
    "            if 'positions' in mentioned_entities:\n",
    "                del mentioned_entities['positions']\n",
    "            \n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'summary': summary,\n",
    "                'publish_time': publish_time,\n",
    "                'content_preview': content,\n",
    "                'mentioned_entities': mentioned_entities,\n",
    "                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"L·ªói khi crawl b√†i b√°o {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong b√†i b√°o\"\"\"\n",
    "        entities = {\n",
    "            'leaders': [],\n",
    "            'organizations': [],\n",
    "            'locations': [],\n",
    "        }\n",
    "        \n",
    "        # C√°c pattern ƒë·ªÉ t√¨m l√£nh ƒë·∫°o v√† ch·ª©c v·ª•\n",
    "        leader_patterns = [\n",
    "            r'T·ªïng [Bb]√≠ th∆∞ ([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±]+ [A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±]+)',\n",
    "            r'Ch·ªß t·ªãch n∆∞·ªõc ([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±]+ [A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±]+)',\n",
    "            r'Th·ªß t∆∞·ªõng ([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±]+ [A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±]+)',\n",
    "            r'B·ªô tr∆∞·ªüng ([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±]+ [A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±]+)',\n",
    "            r'ƒê·∫°i bi·ªÉu ([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±]+ [A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±]+)',\n",
    "            r'T·ªïng th·ªëng ([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-zA-Z\\s]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in leader_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            entities['leaders'].extend(matches)\n",
    "        \n",
    "        # T√¨m c√°c t·ªï ch·ª©c\n",
    "        org_patterns = [\n",
    "            r'(Qu·ªëc h·ªôi|Ch√≠nh ph·ªß|H·ªôi ƒë·ªìng Qu·ªëc gia|B·ªô Ch√≠nh tr·ªã|Ban Ch·∫•p h√†nh Trung ∆∞∆°ng|ASEAN|ƒê·∫£ng C·ªông s·∫£n Vi·ªát Nam)',\n",
    "            r'B·ªô ([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±\\s]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in org_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            if isinstance(matches[0] if matches else '', tuple):\n",
    "                entities['organizations'].extend([match for match in matches])\n",
    "            else:\n",
    "                entities['organizations'].extend(matches)\n",
    "        \n",
    "        # T√¨m ƒë·ªãa danh\n",
    "        location_patterns = [\n",
    "            r'(Vi·ªát Nam|H√† N·ªôi|TP HCM|Qu·∫£ng Ng√£i|Hungary|Ph√°p|Malaysia|ƒê√¥ng Nam √Å)',\n",
    "            r'(t·ªânh|th√†nh ph·ªë) ([A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨ƒê√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞][a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±\\s]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in location_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            if matches:\n",
    "                if isinstance(matches[0], tuple):\n",
    "                    entities['locations'].extend([' '.join(match) for match in matches])\n",
    "                else:\n",
    "                    entities['locations'].extend(matches)\n",
    "        \n",
    "        # Lo·∫°i b·ªè duplicates\n",
    "        for key in entities:\n",
    "            entities[key] = list(set(entities[key]))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def crawl_news(self, limit=5):\n",
    "        \"\"\"Crawl tin t·ª©c ch√≠nh tr·ªã t·ª´ VnExpress\"\"\"\n",
    "        print(f\"B·∫Øt ƒë·∫ßu crawl {limit} b√†i b√°o ch√≠nh tr·ªã t·ª´ VnExpress...\")\n",
    "        \n",
    "        # L·∫•y danh s√°ch link\n",
    "        article_links = self.get_political_news_links(limit)\n",
    "        \n",
    "        if not article_links:\n",
    "            print(\"Kh√¥ng t√¨m th·∫•y b√†i b√°o n√†o!\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"T√¨m th·∫•y {len(article_links)} b√†i b√°o. B·∫Øt ƒë·∫ßu crawl...\")\n",
    "        \n",
    "        articles_data = []\n",
    "        for i, link in enumerate(article_links, 1):\n",
    "            print(f\"ƒêang crawl b√†i {i}/{len(article_links)}: {link}\")\n",
    "            article_data = self.extract_article_info(link)\n",
    "            \n",
    "            if article_data:\n",
    "                articles_data.append(article_data)\n",
    "                print(f\"‚úì Crawl th√†nh c√¥ng: {article_data['title'][:50]}...\")\n",
    "            else:\n",
    "                print(f\"‚úó L·ªói khi crawl b√†i b√°o\")\n",
    "            \n",
    "            # Ngh·ªâ 1 gi√¢y gi·ªØa c√°c request ƒë·ªÉ tr√°nh b·ªã block\n",
    "            time.sleep(1)\n",
    "        \n",
    "        return articles_data\n",
    "    \n",
    "    \n",
    "    def save_to_csv(self, data, filename=\"vnexpress_political_news.csv\"):\n",
    "        \"\"\"L∆∞u d·ªØ li·ªáu v√†o file CSV\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                # Header\n",
    "                writer.writerow([\n",
    "                    'url', 'title', 'summary', 'publish_time', 'content_preview', 'leaders', 'organizations', 'locations', 'crawl_time'\n",
    "                ])\n",
    "                for article in data:\n",
    "                    writer.writerow([\n",
    "                        article.get('url', ''),\n",
    "                        article.get('title', ''),\n",
    "                        article.get('summary', ''),\n",
    "                        article.get('publish_time', ''),\n",
    "                        article.get('content_preview', ''),\n",
    "                        ', '.join(article.get('mentioned_entities', {}).get('leaders', [])),\n",
    "                        ', '.join(article.get('mentioned_entities', {}).get('organizations', [])),\n",
    "                        ', '.join(article.get('mentioned_entities', {}).get('locations', [])),\n",
    "                        article.get('crawl_time', '')\n",
    "                    ])\n",
    "            print(f\"ƒê√£ l∆∞u d·ªØ li·ªáu v√†o file {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"L·ªói khi l∆∞u file CSV: {e}\")\n",
    "    \n",
    "    def print_summary(self, articles_data):\n",
    "        \"\"\"In t√≥m t·∫Øt k·∫øt qu·∫£ crawl\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"T√ìM T·∫ÆT K·∫æT QU·∫¢ CRAWL TIN T·ª®C CH√çNH TR·ªä VNEXPRESS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, article in enumerate(articles_data, 1):\n",
    "            print(f\"\\nüî∏ B√ÄI {i}:\")\n",
    "            print(f\"Ti√™u ƒë·ªÅ: {article['title']}\")\n",
    "            print(f\"URL: {article['url']}\")\n",
    "            print(f\"Th·ªùi gian: {article['publish_time']}\")\n",
    "            print(f\"T√≥m t·∫Øt: {article['summary'][:100]}...\")\n",
    "            \n",
    "            if article['mentioned_entities']['leaders']:\n",
    "                print(f\"L√£nh ƒë·∫°o ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: {', '.join(article['mentioned_entities']['leaders'])}\")\n",
    "            \n",
    "            if article['mentioned_entities']['organizations']:\n",
    "                print(f\"T·ªï ch·ª©c ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: {', '.join(article['mentioned_entities']['organizations'])}\")\n",
    "            \n",
    "            if article['mentioned_entities']['locations']:\n",
    "                print(f\"ƒê·ªãa danh ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p: {', '.join(article['mentioned_entities']['locations'])}\")\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "\n",
    "# S·ª≠ d·ª•ng crawler\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = VnExpressCrawler()\n",
    "    \n",
    "    # Crawl 5 b√†i b√°o ch√≠nh tr·ªã m·ªõi nh·∫•t\n",
    "    articles = crawler.crawl_news(limit=5)\n",
    "    \n",
    "    if articles:\n",
    "        # In t√≥m t·∫Øt\n",
    "        crawler.print_summary(articles)\n",
    "        \n",
    "        # L∆∞u v√†o file CSV\n",
    "        crawler.save_to_csv(articles)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Ho√†n th√†nh crawl {len(articles)} b√†i b√°o!\")\n",
    "        print(\"üìÑ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o file 'vnexpress_political_news.json' v√† 'vnexpress_political_news.csv'\")\n",
    "    else:\n",
    "        print(\"‚ùå Kh√¥ng crawl ƒë∆∞·ª£c b√†i b√°o n√†o!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad862a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
